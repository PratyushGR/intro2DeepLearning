{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport torchtext\nimport time\nimport random\nimport pandas as pd\nimport spacy\n","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:27.539712Z","iopub.execute_input":"2022-05-02T05:23:27.540057Z","iopub.status.idle":"2022-05-02T05:23:38.049274Z","shell.execute_reply.started":"2022-05-02T05:23:27.539936Z","shell.execute_reply":"2022-05-02T05:23:38.048526Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Import Required Libraries & Data Loading","metadata":{}},{"cell_type":"code","source":"#importing the training data\ndf=pd.read_csv('../input/imdb-dataset/IMDB Dataset.csv')\nprint(df.shape)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:38.051017Z","iopub.execute_input":"2022-05-02T05:23:38.051285Z","iopub.status.idle":"2022-05-02T05:23:39.454963Z","shell.execute_reply.started":"2022-05-02T05:23:38.051254Z","shell.execute_reply":"2022-05-02T05:23:39.454244Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(50000, 2)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n5  Probably my all-time favorite movie, a story o...  positive\n6  I sure would like to see a resurrection of a u...  positive\n7  This show was an amazing, fresh & innovative i...  negative\n8  Encouraged by the positive comments about this...  negative\n9  If you like original gut wrenching laughter yo...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Probably my all-time favorite movie, a story o...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I sure would like to see a resurrection of a u...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>This show was an amazing, fresh &amp; innovative i...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Encouraged by the positive comments about this...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>If you like original gut wrenching laughter yo...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"\"\"\"\nsentiment : 0 = negative, 1 = positive \nuse the following to get the sentiment of a sentence :  \nsentiment = 0 if sentiment is negative else 1\n\n\nuse np.where to get the sentiment of a sentence :\n\"\"\"\ndf['sentiment'] = np.where(df['sentiment'] == 'positive', 1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:39.456115Z","iopub.execute_input":"2022-05-02T05:23:39.457136Z","iopub.status.idle":"2022-05-02T05:23:39.472381Z","shell.execute_reply.started":"2022-05-02T05:23:39.457095Z","shell.execute_reply":"2022-05-02T05:23:39.471679Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:39.474562Z","iopub.execute_input":"2022-05-02T05:23:39.475068Z","iopub.status.idle":"2022-05-02T05:23:39.486912Z","shell.execute_reply.started":"2022-05-02T05:23:39.475030Z","shell.execute_reply":"2022-05-02T05:23:39.486191Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                              review  sentiment\n0  One of the other reviewers has mentioned that ...          1\n1  A wonderful little production. <br /><br />The...          1\n2  I thought this was a wonderful way to spend ti...          1\n3  Basically there's a family where a little boy ...          0\n4  Petter Mattei's \"Love in the Time of Money\" is...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.columns = ['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:39.489731Z","iopub.execute_input":"2022-05-02T05:23:39.489915Z","iopub.status.idle":"2022-05-02T05:23:39.495623Z","shell.execute_reply.started":"2022-05-02T05:23:39.489891Z","shell.execute_reply":"2022-05-02T05:23:39.494888Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nLoad the spacy model and load the English language model from https://spacy.io/usage/models\n\"\"\"\nspacy.load('en_core_web_sm')### ADD YOUR SPACY MODEL HERE ###","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:39.497027Z","iopub.execute_input":"2022-05-02T05:23:39.497331Z","iopub.status.idle":"2022-05-02T05:23:40.234955Z","shell.execute_reply.started":"2022-05-02T05:23:39.497295Z","shell.execute_reply":"2022-05-02T05:23:40.234281Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<spacy.lang.en.English at 0x7f119d303150>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Text & label Preparation","metadata":{}},{"cell_type":"code","source":"# Define feature processing\n\"\"\"\nDefine the fields for the data.\n\"\"\"\nTEXT = torchtext.legacy.data.Field(tokenize = 'spacy', tokenizer_language = 'en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:40.236293Z","iopub.execute_input":"2022-05-02T05:23:40.236712Z","iopub.status.idle":"2022-05-02T05:23:40.825367Z","shell.execute_reply.started":"2022-05-02T05:23:40.236674Z","shell.execute_reply":"2022-05-02T05:23:40.824633Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define Label processing\nLABEL = torchtext.legacy.data.LabelField(dtype = torch.long)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:40.826615Z","iopub.execute_input":"2022-05-02T05:23:40.826847Z","iopub.status.idle":"2022-05-02T05:23:40.831668Z","shell.execute_reply.started":"2022-05-02T05:23:40.826815Z","shell.execute_reply":"2022-05-02T05:23:40.830821Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nDefine the fields for the data.\n\"\"\"\n\ndf.to_csv('/tmp/moviedata.csv', index = None)\ndf = pd.read_csv('/tmp/moviedata.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:40.833327Z","iopub.execute_input":"2022-05-02T05:23:40.833768Z","iopub.status.idle":"2022-05-02T05:23:44.110713Z","shell.execute_reply.started":"2022-05-02T05:23:40.833732Z","shell.execute_reply":"2022-05-02T05:23:44.109911Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                    TEXT_COLUMN_NAME  LABEL_COLUMN_NAME\n0  One of the other reviewers has mentioned that ...                  1\n1  A wonderful little production. <br /><br />The...                  1\n2  I thought this was a wonderful way to spend ti...                  1\n3  Basically there's a family where a little boy ...                  0\n4  Petter Mattei's \"Love in the Time of Money\" is...                  1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TEXT_COLUMN_NAME</th>\n      <th>LABEL_COLUMN_NAME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# process the dataset\n\nfield = [('TEXT_COLUMN_NAME', TEXT), ('LABEL_COLUMN_NAME', LABEL)]\ntry: \n  dataset = torchtext.legacy.data.TabularDataset(\n                    path = '/tmp/moviedata.csv', ### ADD YOUR DATASET PATH HERE ###\n                    format = 'csv', ### ADD YOUR DATASET FORMAT HERE ###\n                    skip_header ='True' , ### ADD YOUR SKIP HEADER HERE ### \n                    fields =  field### ADD YOUR FIELDS HERE ### \n  )\nexcept Exception as e:\n  print(e)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:23:44.113917Z","iopub.execute_input":"2022-05-02T05:23:44.114274Z","iopub.status.idle":"2022-05-02T05:24:38.644131Z","shell.execute_reply.started":"2022-05-02T05:23:44.114237Z","shell.execute_reply":"2022-05-02T05:24:38.643379Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vars(dataset.examples[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:38.645569Z","iopub.execute_input":"2022-05-02T05:24:38.645872Z","iopub.status.idle":"2022-05-02T05:24:38.657194Z","shell.execute_reply.started":"2022-05-02T05:24:38.645830Z","shell.execute_reply":"2022-05-02T05:24:38.656527Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'TEXT_COLUMN_NAME': ['One',\n  'of',\n  'the',\n  'other',\n  'reviewers',\n  'has',\n  'mentioned',\n  'that',\n  'after',\n  'watching',\n  'just',\n  '1',\n  'Oz',\n  'episode',\n  'you',\n  \"'ll\",\n  'be',\n  'hooked',\n  '.',\n  'They',\n  'are',\n  'right',\n  ',',\n  'as',\n  'this',\n  'is',\n  'exactly',\n  'what',\n  'happened',\n  'with',\n  'me.<br',\n  '/><br',\n  '/>The',\n  'first',\n  'thing',\n  'that',\n  'struck',\n  'me',\n  'about',\n  'Oz',\n  'was',\n  'its',\n  'brutality',\n  'and',\n  'unflinching',\n  'scenes',\n  'of',\n  'violence',\n  ',',\n  'which',\n  'set',\n  'in',\n  'right',\n  'from',\n  'the',\n  'word',\n  'GO',\n  '.',\n  'Trust',\n  'me',\n  ',',\n  'this',\n  'is',\n  'not',\n  'a',\n  'show',\n  'for',\n  'the',\n  'faint',\n  'hearted',\n  'or',\n  'timid',\n  '.',\n  'This',\n  'show',\n  'pulls',\n  'no',\n  'punches',\n  'with',\n  'regards',\n  'to',\n  'drugs',\n  ',',\n  'sex',\n  'or',\n  'violence',\n  '.',\n  'Its',\n  'is',\n  'hardcore',\n  ',',\n  'in',\n  'the',\n  'classic',\n  'use',\n  'of',\n  'the',\n  'word.<br',\n  '/><br',\n  '/>It',\n  'is',\n  'called',\n  'OZ',\n  'as',\n  'that',\n  'is',\n  'the',\n  'nickname',\n  'given',\n  'to',\n  'the',\n  'Oswald',\n  'Maximum',\n  'Security',\n  'State',\n  'Penitentary',\n  '.',\n  'It',\n  'focuses',\n  'mainly',\n  'on',\n  'Emerald',\n  'City',\n  ',',\n  'an',\n  'experimental',\n  'section',\n  'of',\n  'the',\n  'prison',\n  'where',\n  'all',\n  'the',\n  'cells',\n  'have',\n  'glass',\n  'fronts',\n  'and',\n  'face',\n  'inwards',\n  ',',\n  'so',\n  'privacy',\n  'is',\n  'not',\n  'high',\n  'on',\n  'the',\n  'agenda',\n  '.',\n  'Em',\n  'City',\n  'is',\n  'home',\n  'to',\n  'many',\n  '..',\n  'Aryans',\n  ',',\n  'Muslims',\n  ',',\n  'gangstas',\n  ',',\n  'Latinos',\n  ',',\n  'Christians',\n  ',',\n  'Italians',\n  ',',\n  'Irish',\n  'and',\n  'more',\n  '....',\n  'so',\n  'scuffles',\n  ',',\n  'death',\n  'stares',\n  ',',\n  'dodgy',\n  'dealings',\n  'and',\n  'shady',\n  'agreements',\n  'are',\n  'never',\n  'far',\n  'away.<br',\n  '/><br',\n  '/>I',\n  'would',\n  'say',\n  'the',\n  'main',\n  'appeal',\n  'of',\n  'the',\n  'show',\n  'is',\n  'due',\n  'to',\n  'the',\n  'fact',\n  'that',\n  'it',\n  'goes',\n  'where',\n  'other',\n  'shows',\n  'would',\n  \"n't\",\n  'dare',\n  '.',\n  'Forget',\n  'pretty',\n  'pictures',\n  'painted',\n  'for',\n  'mainstream',\n  'audiences',\n  ',',\n  'forget',\n  'charm',\n  ',',\n  'forget',\n  'romance',\n  '...',\n  'OZ',\n  'does',\n  \"n't\",\n  'mess',\n  'around',\n  '.',\n  'The',\n  'first',\n  'episode',\n  'I',\n  'ever',\n  'saw',\n  'struck',\n  'me',\n  'as',\n  'so',\n  'nasty',\n  'it',\n  'was',\n  'surreal',\n  ',',\n  'I',\n  'could',\n  \"n't\",\n  'say',\n  'I',\n  'was',\n  'ready',\n  'for',\n  'it',\n  ',',\n  'but',\n  'as',\n  'I',\n  'watched',\n  'more',\n  ',',\n  'I',\n  'developed',\n  'a',\n  'taste',\n  'for',\n  'Oz',\n  ',',\n  'and',\n  'got',\n  'accustomed',\n  'to',\n  'the',\n  'high',\n  'levels',\n  'of',\n  'graphic',\n  'violence',\n  '.',\n  'Not',\n  'just',\n  'violence',\n  ',',\n  'but',\n  'injustice',\n  '(',\n  'crooked',\n  'guards',\n  'who',\n  \"'ll\",\n  'be',\n  'sold',\n  'out',\n  'for',\n  'a',\n  'nickel',\n  ',',\n  'inmates',\n  'who',\n  \"'ll\",\n  'kill',\n  'on',\n  'order',\n  'and',\n  'get',\n  'away',\n  'with',\n  'it',\n  ',',\n  'well',\n  'mannered',\n  ',',\n  'middle',\n  'class',\n  'inmates',\n  'being',\n  'turned',\n  'into',\n  'prison',\n  'bitches',\n  'due',\n  'to',\n  'their',\n  'lack',\n  'of',\n  'street',\n  'skills',\n  'or',\n  'prison',\n  'experience',\n  ')',\n  'Watching',\n  'Oz',\n  ',',\n  'you',\n  'may',\n  'become',\n  'comfortable',\n  'with',\n  'what',\n  'is',\n  'uncomfortable',\n  'viewing',\n  '....',\n  'thats',\n  'if',\n  'you',\n  'can',\n  'get',\n  'in',\n  'touch',\n  'with',\n  'your',\n  'darker',\n  'side',\n  '.'],\n 'LABEL_COLUMN_NAME': '1'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Split","metadata":{}},{"cell_type":"code","source":"# Split dataset into train and test set\nRANDOM_SEED = 123\ntrain_data, test_data = dataset.split(split_ratio = [0.8, 0.2], random_state = random.seed(RANDOM_SEED))\n\nprint('Length of train data', len(train_data))\nprint('Length of test data', len(test_data))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:38.658573Z","iopub.execute_input":"2022-05-02T05:24:38.658945Z","iopub.status.idle":"2022-05-02T05:24:38.734500Z","shell.execute_reply.started":"2022-05-02T05:24:38.658907Z","shell.execute_reply":"2022-05-02T05:24:38.733663Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Length of train data 40000\nLength of test data 10000\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data, val_data = train_data.split(split_ratio = [0.85, 0.15], random_state = random.seed(RANDOM_SEED))\n\nprint('Length of train data', len(train_data))\nprint('Length of valid data', len(val_data))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:38.735752Z","iopub.execute_input":"2022-05-02T05:24:38.736002Z","iopub.status.idle":"2022-05-02T05:24:38.794336Z","shell.execute_reply.started":"2022-05-02T05:24:38.735950Z","shell.execute_reply":"2022-05-02T05:24:38.793547Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Length of train data 34000\nLength of valid data 6000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Observation after Tokenization","metadata":{}},{"cell_type":"code","source":"# Look at first traning example\n\nprint(vars(train_data.examples[2000]))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:38.795511Z","iopub.execute_input":"2022-05-02T05:24:38.796230Z","iopub.status.idle":"2022-05-02T05:24:38.801275Z","shell.execute_reply.started":"2022-05-02T05:24:38.796189Z","shell.execute_reply":"2022-05-02T05:24:38.800539Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"{'TEXT_COLUMN_NAME': ['Flipping', 'through', 'the', 'channels', 'I', 'was', 'lucky', 'enough', 'to', 'stumble', 'upon', 'the', 'beginning', 'of', 'this', 'movie', '.', 'I', 'must', 'admit', 'that', 'it', 'grabbed', 'my', 'attention', 'almost', 'immediately', '.', 'I', 'love', 'older', 'films', 'and', 'this', 'is', 'or', 'should', 'be', 'considered', 'a', 'classic', '!', 'One', 'of', 'the', 'most', 'wonderful', 'rarities', 'of', 'this', 'movie', 'is', 'that', 'the', 'main', 'character', 'was', 'not', 'only', 'female', 'but', 'she', 'was', 'also', 'a', 'bad', 'girl', '.', 'I', 'highly', 'recommend', 'this', 'movie', '!'], 'LABEL_COLUMN_NAME': '1'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Build Vocabulary\nVOCABULARY_SIZE = 20000\nTEXT.build_vocab(train_data, max_size = VOCABULARY_SIZE)\nLABEL.build_vocab(train_data)\n\nprint(f'vocabulary size: {len(TEXT.vocab)}')\nprint(f'Label Size: {len(LABEL.vocab)}')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:38.802493Z","iopub.execute_input":"2022-05-02T05:24:38.803109Z","iopub.status.idle":"2022-05-02T05:24:40.543192Z","shell.execute_reply.started":"2022-05-02T05:24:38.803072Z","shell.execute_reply":"2022-05-02T05:24:40.541706Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"vocabulary size: 20002\nLabel Size: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"a= vars(TEXT.vocab)\nx= a['freqs']\nsorted_freqs=[(l,k) for k,l in sorted([(j,i) for i,j in x.items()], reverse=True)]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:40.544331Z","iopub.execute_input":"2022-05-02T05:24:40.544597Z","iopub.status.idle":"2022-05-02T05:24:40.885013Z","shell.execute_reply.started":"2022-05-02T05:24:40.544562Z","shell.execute_reply":"2022-05-02T05:24:40.884275Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":" 2 extra value in vocabulary is because added (unknown) and (padding)","metadata":{}},{"cell_type":"code","source":"# Print the most common words: Use the most_common method of the TEXT vocabulary\nmost_common_words = sorted_freqs[0:20]\nprint(most_common_words)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:40.886343Z","iopub.execute_input":"2022-05-02T05:24:40.886585Z","iopub.status.idle":"2022-05-02T05:24:40.894233Z","shell.execute_reply.started":"2022-05-02T05:24:40.886553Z","shell.execute_reply":"2022-05-02T05:24:40.893285Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[('the', 390972), (',', 369444), ('.', 318509), ('a', 210502), ('and', 210008), ('of', 194659), ('to', 180163), ('is', 145895), ('in', 118266), ('I', 105681), ('it', 103588), ('that', 93995), ('\"', 85535), (\"'s\", 83149), ('this', 81775), ('-', 71249), ('/><br', 68787), ('was', 67372), ('as', 57734), ('movie', 57572)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Token corresponding to first 10 Indices\n\nprint(TEXT.vocab.itos[:20]) #itos = Integer to string","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:40.895957Z","iopub.execute_input":"2022-05-02T05:24:40.896396Z","iopub.status.idle":"2022-05-02T05:24:40.902381Z","shell.execute_reply.started":"2022-05-02T05:24:40.896189Z","shell.execute_reply":"2022-05-02T05:24:40.900873Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'it', 'that', '\"', \"'s\", 'this', '-', '/><br', 'was']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preparation for Batch wise Implimentation","metadata":{}},{"cell_type":"code","source":"# Define Dataloader\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrain_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n        (train_data, val_data, test_data), ### ADD YOUR SPLIT DATA HERE (Make sure you add it in a tuple) ###\n        batch_size = 128, ### ADD YOUR BATCH SIZE HERE ###\n        sort_within_batch = True, ### ADD YOUR SORT WITHIN BATCH HERE ### \n        sort_key = lambda x : len(x.TEXT_COLUMN_NAME), \n        device = DEVICE\n    )","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:40.904057Z","iopub.execute_input":"2022-05-02T05:24:40.904377Z","iopub.status.idle":"2022-05-02T05:24:40.912713Z","shell.execute_reply.started":"2022-05-02T05:24:40.904342Z","shell.execute_reply":"2022-05-02T05:24:40.912034Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Testing the iterators (note that the number of rows depends on the longest document in the respective batch):\n\nprint('Train')\nfor batch in train_loader:\n    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n    break\n    \nprint('\\nValid:')\nfor batch in valid_loader:\n    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n    break\n    \nprint('\\nTest:')\nfor batch in test_loader:\n    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n    break","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:40.914238Z","iopub.execute_input":"2022-05-02T05:24:40.914821Z","iopub.status.idle":"2022-05-02T05:24:48.131391Z","shell.execute_reply.started":"2022-05-02T05:24:40.914784Z","shell.execute_reply":"2022-05-02T05:24:48.130665Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Train\nText matrix size: torch.Size([250, 128])\nTarget vector size: torch.Size([128])\n\nValid:\nText matrix size: torch.Size([56, 128])\nTarget vector size: torch.Size([128])\n\nTest:\nText matrix size: torch.Size([50, 128])\nTarget vector size: torch.Size([128])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"from torch import nn\nclass RNN(nn.Module):\n  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n    super().__init__() #to call the functions in the superclass\n    self.embedding = nn.Embedding(input_dim, embedding_dim) #Embedding layer to create dense vector instead of sparse matrix\n    self.rnn = nn.RNN(embedding_dim, hidden_dim) \n    self.hidden_fc = nn.Linear(hidden_dim,hidden_dim)\n    self.out_fc = nn.Linear(hidden_dim, output_dim)\n    self.dropout = nn.Dropout(0.3)\n  def forward(self, text):\n    embedded = self.embedding(text)\n    output, hidden = self.rnn(embedded)   \n    hidden = self.dropout(hidden[-1,:,:])\n    hidden = F.relu(self.hidden_fc(hidden))\n    return self.out_fc(hidden)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T06:10:47.463466Z","iopub.execute_input":"2022-05-02T06:10:47.463719Z","iopub.status.idle":"2022-05-02T06:10:47.470764Z","shell.execute_reply.started":"2022-05-02T06:10:47.463691Z","shell.execute_reply":"2022-05-02T06:10:47.470072Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"class RNN1(torch.nn.Module):\n    \n    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n        super().__init__()\n        ### ADD YOUR CODE HERE ###\n        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n        \n        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim)\n        \n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        # embedding and LSTM layers\n        \n        ### END YOUR CODE ### \n\n    def forward(self, text):\n        ### ADD YOUR CODE HERE ###\n        # text dim: [sentence length, batch size]\n        # embedded dim: [sentence length, batch size, embedding dim]\n        embedded = self.embedding(text)\n        output, hidden = self.rnn(embedded)   \n        \n        # output dim: [sentence length, batch size, hidden dim]\n        # hidden dim: [1, batch size, hidden dim]\n\n        # hidden dim: [batch size, hidden dim]\n        assert torch.equal(output[-1,:,:],hidden.squeeze(0))\n        \n        ### END YOUR CODE ###\n        #output = \n        return self.fc(hidden.squeeze(0))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T06:23:58.542657Z","iopub.execute_input":"2022-05-02T06:23:58.542914Z","iopub.status.idle":"2022-05-02T06:23:58.550642Z","shell.execute_reply.started":"2022-05-02T06:23:58.542884Z","shell.execute_reply":"2022-05-02T06:23:58.549673Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(RANDOM_SEED)\n\nmodel = RNN1(input_dim= len(TEXT.vocab), ### ADD YOUR INPUT DIM HERE. This can be the length of your vocabulary or the embedding dim ###\n            embedding_dim=400, ### ADD YOUR EMBEDDING DIM HERE ###\n            hidden_dim=128, ### ADD YOUR HIDDEN DIM HERE ###\n            output_dim=2  ### ADD NUMBER OF CLASSES HERE ###\n)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(DEVICE)\noptimizer =  torch.optim.Adam(model.parameters(), lr=1e-3)### ADD YOUR OPTIMIZER HERE ###","metadata":{"execution":{"iopub.status.busy":"2022-05-02T06:28:02.273662Z","iopub.execute_input":"2022-05-02T06:28:02.273918Z","iopub.status.idle":"2022-05-02T06:28:02.363011Z","shell.execute_reply.started":"2022-05-02T06:28:02.273889Z","shell.execute_reply":"2022-05-02T06:28:02.362290Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"# Define Accuracy","metadata":{}},{"cell_type":"code","source":"def compute_accuracy(model, data_loader, device):\n\n    with torch.no_grad():\n\n        correct_pred, num_examples = 0, 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits = model(features)\n            _, predicted_labels = torch.max(logits, 1)\n\n            num_examples += targets.size(0)\n            correct_pred += (predicted_labels == targets).sum()\n    return correct_pred.float()/num_examples * 100","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:24:48.949303Z","iopub.execute_input":"2022-05-02T05:24:48.949564Z","iopub.status.idle":"2022-05-02T05:24:48.956780Z","shell.execute_reply.started":"2022-05-02T05:24:48.949530Z","shell.execute_reply":"2022-05-02T05:24:48.956071Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Model Run","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nNUM_EPOCHS = 50\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n#criterion = torch.nn.BCEWithLogitsLoss()\nmodel = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    for batch_idx, batch_data in enumerate(train_loader):\n        text = batch_data.TEXT_COLUMN_NAME.to(DEVICE)\n        labels = batch_data.LABEL_COLUMN_NAME.to(DEVICE)\n\n        ### FORWARD AND BACK PROP\n                \n        predictions = model(text)\n        \n        loss = F.cross_entropy(predictions, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step()\n        ### UPDATE MODEL PARAMETERS\n        \n        \n        ### LOGGING\n        if not batch_idx % 50:\n            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n                   f'Loss: {loss:.4f}')     \n    with torch.set_grad_enabled(False):\n        print(f'training accuracy: '\n              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n              f'\\nvalid accuracy: '\n              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n        \n    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n    \nprint(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\nprint(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T06:32:09.983396Z","iopub.execute_input":"2022-05-02T06:32:09.983651Z","iopub.status.idle":"2022-05-02T06:41:05.387865Z","shell.execute_reply.started":"2022-05-02T06:32:09.983621Z","shell.execute_reply":"2022-05-02T06:41:05.387141Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Epoch: 001/050 | Batch 000/266 | Loss: 0.3095\nEpoch: 001/050 | Batch 050/266 | Loss: 0.3761\nEpoch: 001/050 | Batch 100/266 | Loss: 0.5528\nEpoch: 001/050 | Batch 150/266 | Loss: 0.4029\nEpoch: 001/050 | Batch 200/266 | Loss: 0.5393\nEpoch: 001/050 | Batch 250/266 | Loss: 0.7313\ntraining accuracy: 62.65%\nvalid accuracy: 57.98%\nTime elapsed: 0.18 min\nEpoch: 002/050 | Batch 000/266 | Loss: 0.7223\nEpoch: 002/050 | Batch 050/266 | Loss: 0.6451\nEpoch: 002/050 | Batch 100/266 | Loss: 0.5268\nEpoch: 002/050 | Batch 150/266 | Loss: 0.5357\nEpoch: 002/050 | Batch 200/266 | Loss: 0.5202\nEpoch: 002/050 | Batch 250/266 | Loss: 0.3992\ntraining accuracy: 80.88%\nvalid accuracy: 70.68%\nTime elapsed: 0.35 min\nEpoch: 003/050 | Batch 000/266 | Loss: 0.4783\nEpoch: 003/050 | Batch 050/266 | Loss: 0.3261\nEpoch: 003/050 | Batch 100/266 | Loss: 0.4176\nEpoch: 003/050 | Batch 150/266 | Loss: 0.6314\nEpoch: 003/050 | Batch 200/266 | Loss: 0.9468\nEpoch: 003/050 | Batch 250/266 | Loss: 0.5817\ntraining accuracy: 74.89%\nvalid accuracy: 63.93%\nTime elapsed: 0.53 min\nEpoch: 004/050 | Batch 000/266 | Loss: 0.5133\nEpoch: 004/050 | Batch 050/266 | Loss: 0.5373\nEpoch: 004/050 | Batch 100/266 | Loss: 0.5055\nEpoch: 004/050 | Batch 150/266 | Loss: 0.4593\nEpoch: 004/050 | Batch 200/266 | Loss: 0.5966\nEpoch: 004/050 | Batch 250/266 | Loss: 0.4836\ntraining accuracy: 81.62%\nvalid accuracy: 72.58%\nTime elapsed: 0.70 min\nEpoch: 005/050 | Batch 000/266 | Loss: 0.4654\nEpoch: 005/050 | Batch 050/266 | Loss: 0.6291\nEpoch: 005/050 | Batch 100/266 | Loss: 0.3563\nEpoch: 005/050 | Batch 150/266 | Loss: 0.4192\nEpoch: 005/050 | Batch 200/266 | Loss: 0.4210\nEpoch: 005/050 | Batch 250/266 | Loss: 0.4281\ntraining accuracy: 80.91%\nvalid accuracy: 73.23%\nTime elapsed: 0.88 min\nEpoch: 006/050 | Batch 000/266 | Loss: 0.4255\nEpoch: 006/050 | Batch 050/266 | Loss: 0.4693\nEpoch: 006/050 | Batch 100/266 | Loss: 0.3744\nEpoch: 006/050 | Batch 150/266 | Loss: 0.4484\nEpoch: 006/050 | Batch 200/266 | Loss: 0.3854\nEpoch: 006/050 | Batch 250/266 | Loss: 0.4672\ntraining accuracy: 82.90%\nvalid accuracy: 73.90%\nTime elapsed: 1.06 min\nEpoch: 007/050 | Batch 000/266 | Loss: 0.3104\nEpoch: 007/050 | Batch 050/266 | Loss: 0.3707\nEpoch: 007/050 | Batch 100/266 | Loss: 0.5217\nEpoch: 007/050 | Batch 150/266 | Loss: 0.5150\nEpoch: 007/050 | Batch 200/266 | Loss: 0.4232\nEpoch: 007/050 | Batch 250/266 | Loss: 0.4958\ntraining accuracy: 83.47%\nvalid accuracy: 72.73%\nTime elapsed: 1.24 min\nEpoch: 008/050 | Batch 000/266 | Loss: 0.3965\nEpoch: 008/050 | Batch 050/266 | Loss: 0.3268\nEpoch: 008/050 | Batch 100/266 | Loss: 0.2836\nEpoch: 008/050 | Batch 150/266 | Loss: 0.5604\nEpoch: 008/050 | Batch 200/266 | Loss: 0.3857\nEpoch: 008/050 | Batch 250/266 | Loss: 0.5504\ntraining accuracy: 61.34%\nvalid accuracy: 53.53%\nTime elapsed: 1.42 min\nEpoch: 009/050 | Batch 000/266 | Loss: 0.5357\nEpoch: 009/050 | Batch 050/266 | Loss: 0.4685\nEpoch: 009/050 | Batch 100/266 | Loss: 0.4263\nEpoch: 009/050 | Batch 150/266 | Loss: 0.5193\nEpoch: 009/050 | Batch 200/266 | Loss: 0.4872\nEpoch: 009/050 | Batch 250/266 | Loss: 0.3677\ntraining accuracy: 77.82%\nvalid accuracy: 68.83%\nTime elapsed: 1.60 min\nEpoch: 010/050 | Batch 000/266 | Loss: 0.4210\nEpoch: 010/050 | Batch 050/266 | Loss: 0.2756\nEpoch: 010/050 | Batch 100/266 | Loss: 0.2422\nEpoch: 010/050 | Batch 150/266 | Loss: 0.2894\nEpoch: 010/050 | Batch 200/266 | Loss: 0.3605\nEpoch: 010/050 | Batch 250/266 | Loss: 0.3426\ntraining accuracy: 84.09%\nvalid accuracy: 73.23%\nTime elapsed: 1.78 min\nEpoch: 011/050 | Batch 000/266 | Loss: 0.3623\nEpoch: 011/050 | Batch 050/266 | Loss: 0.2647\nEpoch: 011/050 | Batch 100/266 | Loss: 0.2631\nEpoch: 011/050 | Batch 150/266 | Loss: 0.3781\nEpoch: 011/050 | Batch 200/266 | Loss: 0.6787\nEpoch: 011/050 | Batch 250/266 | Loss: 0.3888\ntraining accuracy: 82.76%\nvalid accuracy: 73.68%\nTime elapsed: 1.95 min\nEpoch: 012/050 | Batch 000/266 | Loss: 0.2546\nEpoch: 012/050 | Batch 050/266 | Loss: 0.3207\nEpoch: 012/050 | Batch 100/266 | Loss: 0.5872\nEpoch: 012/050 | Batch 150/266 | Loss: 0.4086\nEpoch: 012/050 | Batch 200/266 | Loss: 0.3185\nEpoch: 012/050 | Batch 250/266 | Loss: 0.3711\ntraining accuracy: 84.43%\nvalid accuracy: 73.78%\nTime elapsed: 2.13 min\nEpoch: 013/050 | Batch 000/266 | Loss: 0.3985\nEpoch: 013/050 | Batch 050/266 | Loss: 0.6925\nEpoch: 013/050 | Batch 100/266 | Loss: 0.3165\nEpoch: 013/050 | Batch 150/266 | Loss: 0.2436\nEpoch: 013/050 | Batch 200/266 | Loss: 0.2881\nEpoch: 013/050 | Batch 250/266 | Loss: 0.5267\ntraining accuracy: 71.95%\nvalid accuracy: 70.37%\nTime elapsed: 2.31 min\nEpoch: 014/050 | Batch 000/266 | Loss: 0.6437\nEpoch: 014/050 | Batch 050/266 | Loss: 0.5926\nEpoch: 014/050 | Batch 100/266 | Loss: 0.5171\nEpoch: 014/050 | Batch 150/266 | Loss: 0.3416\nEpoch: 014/050 | Batch 200/266 | Loss: 0.5046\nEpoch: 014/050 | Batch 250/266 | Loss: 0.4534\ntraining accuracy: 80.64%\nvalid accuracy: 71.57%\nTime elapsed: 2.48 min\nEpoch: 015/050 | Batch 000/266 | Loss: 0.4024\nEpoch: 015/050 | Batch 050/266 | Loss: 0.3654\nEpoch: 015/050 | Batch 100/266 | Loss: 0.2964\nEpoch: 015/050 | Batch 150/266 | Loss: 0.4206\nEpoch: 015/050 | Batch 200/266 | Loss: 0.4792\nEpoch: 015/050 | Batch 250/266 | Loss: 0.6732\ntraining accuracy: 70.64%\nvalid accuracy: 68.58%\nTime elapsed: 2.66 min\nEpoch: 016/050 | Batch 000/266 | Loss: 0.5562\nEpoch: 016/050 | Batch 050/266 | Loss: 0.6861\nEpoch: 016/050 | Batch 100/266 | Loss: 0.5977\nEpoch: 016/050 | Batch 150/266 | Loss: 0.4943\nEpoch: 016/050 | Batch 200/266 | Loss: 0.7183\nEpoch: 016/050 | Batch 250/266 | Loss: 0.6043\ntraining accuracy: 64.24%\nvalid accuracy: 57.02%\nTime elapsed: 2.84 min\nEpoch: 017/050 | Batch 000/266 | Loss: 0.6470\nEpoch: 017/050 | Batch 050/266 | Loss: 0.4584\nEpoch: 017/050 | Batch 100/266 | Loss: 0.4399\nEpoch: 017/050 | Batch 150/266 | Loss: 0.4643\nEpoch: 017/050 | Batch 200/266 | Loss: 0.5422\nEpoch: 017/050 | Batch 250/266 | Loss: 0.5314\ntraining accuracy: 79.55%\nvalid accuracy: 71.98%\nTime elapsed: 3.01 min\nEpoch: 018/050 | Batch 000/266 | Loss: 0.3700\nEpoch: 018/050 | Batch 050/266 | Loss: 0.3863\nEpoch: 018/050 | Batch 100/266 | Loss: 0.5012\nEpoch: 018/050 | Batch 150/266 | Loss: 0.4550\nEpoch: 018/050 | Batch 200/266 | Loss: 0.4601\nEpoch: 018/050 | Batch 250/266 | Loss: 0.5384\ntraining accuracy: 80.74%\nvalid accuracy: 72.33%\nTime elapsed: 3.19 min\nEpoch: 019/050 | Batch 000/266 | Loss: 0.4117\nEpoch: 019/050 | Batch 050/266 | Loss: 0.4326\nEpoch: 019/050 | Batch 100/266 | Loss: 0.4994\nEpoch: 019/050 | Batch 150/266 | Loss: 0.5258\nEpoch: 019/050 | Batch 200/266 | Loss: 0.4935\nEpoch: 019/050 | Batch 250/266 | Loss: 0.4803\ntraining accuracy: 81.10%\nvalid accuracy: 74.25%\nTime elapsed: 3.37 min\nEpoch: 020/050 | Batch 000/266 | Loss: 0.4492\nEpoch: 020/050 | Batch 050/266 | Loss: 0.5141\nEpoch: 020/050 | Batch 100/266 | Loss: 0.4739\nEpoch: 020/050 | Batch 150/266 | Loss: 0.5666\nEpoch: 020/050 | Batch 200/266 | Loss: 0.4713\nEpoch: 020/050 | Batch 250/266 | Loss: 0.5158\ntraining accuracy: 82.10%\nvalid accuracy: 73.32%\nTime elapsed: 3.54 min\nEpoch: 021/050 | Batch 000/266 | Loss: 0.3559\nEpoch: 021/050 | Batch 050/266 | Loss: 0.3156\nEpoch: 021/050 | Batch 100/266 | Loss: 0.4220\nEpoch: 021/050 | Batch 150/266 | Loss: 0.3904\nEpoch: 021/050 | Batch 200/266 | Loss: 0.5909\nEpoch: 021/050 | Batch 250/266 | Loss: 0.4436\ntraining accuracy: 80.39%\nvalid accuracy: 67.97%\nTime elapsed: 3.72 min\nEpoch: 022/050 | Batch 000/266 | Loss: 0.4116\nEpoch: 022/050 | Batch 050/266 | Loss: 0.4630\nEpoch: 022/050 | Batch 100/266 | Loss: 0.4867\nEpoch: 022/050 | Batch 150/266 | Loss: 0.3199\nEpoch: 022/050 | Batch 200/266 | Loss: 0.5390\nEpoch: 022/050 | Batch 250/266 | Loss: 0.4430\ntraining accuracy: 82.48%\nvalid accuracy: 72.77%\nTime elapsed: 3.89 min\nEpoch: 023/050 | Batch 000/266 | Loss: 0.5672\nEpoch: 023/050 | Batch 050/266 | Loss: 0.4198\nEpoch: 023/050 | Batch 100/266 | Loss: 0.3566\nEpoch: 023/050 | Batch 150/266 | Loss: 0.5594\nEpoch: 023/050 | Batch 200/266 | Loss: 0.4804\nEpoch: 023/050 | Batch 250/266 | Loss: 0.4040\ntraining accuracy: 80.70%\nvalid accuracy: 70.53%\nTime elapsed: 4.07 min\nEpoch: 024/050 | Batch 000/266 | Loss: 0.4695\nEpoch: 024/050 | Batch 050/266 | Loss: 0.5572\nEpoch: 024/050 | Batch 100/266 | Loss: 0.4614\nEpoch: 024/050 | Batch 150/266 | Loss: 0.3900\nEpoch: 024/050 | Batch 200/266 | Loss: 0.4681\nEpoch: 024/050 | Batch 250/266 | Loss: 0.5706\ntraining accuracy: 84.24%\nvalid accuracy: 72.83%\nTime elapsed: 4.25 min\nEpoch: 025/050 | Batch 000/266 | Loss: 0.6339\nEpoch: 025/050 | Batch 050/266 | Loss: 0.4475\nEpoch: 025/050 | Batch 100/266 | Loss: 0.5891\nEpoch: 025/050 | Batch 150/266 | Loss: 0.4552\nEpoch: 025/050 | Batch 200/266 | Loss: 0.3570\nEpoch: 025/050 | Batch 250/266 | Loss: 0.4427\ntraining accuracy: 85.26%\nvalid accuracy: 72.73%\nTime elapsed: 4.43 min\nEpoch: 026/050 | Batch 000/266 | Loss: 0.2380\nEpoch: 026/050 | Batch 050/266 | Loss: 0.3068\nEpoch: 026/050 | Batch 100/266 | Loss: 0.3728\nEpoch: 026/050 | Batch 150/266 | Loss: 0.2943\nEpoch: 026/050 | Batch 200/266 | Loss: 0.3840\nEpoch: 026/050 | Batch 250/266 | Loss: 0.3159\ntraining accuracy: 86.00%\nvalid accuracy: 73.03%\nTime elapsed: 4.60 min\nEpoch: 027/050 | Batch 000/266 | Loss: 0.2978\nEpoch: 027/050 | Batch 050/266 | Loss: 0.3519\nEpoch: 027/050 | Batch 100/266 | Loss: 0.3211\nEpoch: 027/050 | Batch 150/266 | Loss: 0.5166\nEpoch: 027/050 | Batch 200/266 | Loss: 0.4523\nEpoch: 027/050 | Batch 250/266 | Loss: 0.4038\ntraining accuracy: 83.50%\nvalid accuracy: 71.63%\nTime elapsed: 4.78 min\nEpoch: 028/050 | Batch 000/266 | Loss: 0.3641\nEpoch: 028/050 | Batch 050/266 | Loss: 0.3866\nEpoch: 028/050 | Batch 100/266 | Loss: 0.4379\nEpoch: 028/050 | Batch 150/266 | Loss: 0.2986\nEpoch: 028/050 | Batch 200/266 | Loss: 0.3804\nEpoch: 028/050 | Batch 250/266 | Loss: 0.4415\ntraining accuracy: 79.86%\nvalid accuracy: 66.37%\nTime elapsed: 4.96 min\nEpoch: 029/050 | Batch 000/266 | Loss: 0.6440\nEpoch: 029/050 | Batch 050/266 | Loss: 0.4450\nEpoch: 029/050 | Batch 100/266 | Loss: 0.4108\nEpoch: 029/050 | Batch 150/266 | Loss: 0.6694\nEpoch: 029/050 | Batch 200/266 | Loss: 0.3964\nEpoch: 029/050 | Batch 250/266 | Loss: 0.4869\ntraining accuracy: 79.23%\nvalid accuracy: 68.83%\nTime elapsed: 5.13 min\nEpoch: 030/050 | Batch 000/266 | Loss: 0.5765\nEpoch: 030/050 | Batch 050/266 | Loss: 0.4903\nEpoch: 030/050 | Batch 100/266 | Loss: 0.4924\nEpoch: 030/050 | Batch 150/266 | Loss: 0.3620\nEpoch: 030/050 | Batch 200/266 | Loss: 0.3382\nEpoch: 030/050 | Batch 250/266 | Loss: 0.4146\ntraining accuracy: 82.38%\nvalid accuracy: 71.07%\nTime elapsed: 5.31 min\nEpoch: 031/050 | Batch 000/266 | Loss: 0.5580\nEpoch: 031/050 | Batch 050/266 | Loss: 0.4485\nEpoch: 031/050 | Batch 100/266 | Loss: 0.4010\nEpoch: 031/050 | Batch 150/266 | Loss: 0.4760\nEpoch: 031/050 | Batch 200/266 | Loss: 0.4395\nEpoch: 031/050 | Batch 250/266 | Loss: 0.4375\ntraining accuracy: 83.69%\nvalid accuracy: 71.48%\nTime elapsed: 5.49 min\nEpoch: 032/050 | Batch 000/266 | Loss: 0.3074\nEpoch: 032/050 | Batch 050/266 | Loss: 0.4358\nEpoch: 032/050 | Batch 100/266 | Loss: 0.3681\nEpoch: 032/050 | Batch 150/266 | Loss: 0.5780\nEpoch: 032/050 | Batch 200/266 | Loss: 0.3832\nEpoch: 032/050 | Batch 250/266 | Loss: 0.3566\ntraining accuracy: 84.66%\nvalid accuracy: 71.78%\nTime elapsed: 5.67 min\nEpoch: 033/050 | Batch 000/266 | Loss: 0.5144\nEpoch: 033/050 | Batch 050/266 | Loss: 0.3792\nEpoch: 033/050 | Batch 100/266 | Loss: 1.0300\nEpoch: 033/050 | Batch 150/266 | Loss: 0.4084\nEpoch: 033/050 | Batch 200/266 | Loss: 0.4665\nEpoch: 033/050 | Batch 250/266 | Loss: 0.5760\ntraining accuracy: 85.30%\nvalid accuracy: 71.80%\nTime elapsed: 5.85 min\nEpoch: 034/050 | Batch 000/266 | Loss: 0.4821\nEpoch: 034/050 | Batch 050/266 | Loss: 0.2826\nEpoch: 034/050 | Batch 100/266 | Loss: 0.2640\nEpoch: 034/050 | Batch 150/266 | Loss: 0.3733\nEpoch: 034/050 | Batch 200/266 | Loss: 0.3403\nEpoch: 034/050 | Batch 250/266 | Loss: 0.3910\ntraining accuracy: 85.36%\nvalid accuracy: 71.43%\nTime elapsed: 6.03 min\nEpoch: 035/050 | Batch 000/266 | Loss: 0.2953\nEpoch: 035/050 | Batch 050/266 | Loss: 0.3598\nEpoch: 035/050 | Batch 100/266 | Loss: 0.3223\nEpoch: 035/050 | Batch 150/266 | Loss: 0.4086\nEpoch: 035/050 | Batch 200/266 | Loss: 0.3385\nEpoch: 035/050 | Batch 250/266 | Loss: 0.4257\ntraining accuracy: 86.63%\nvalid accuracy: 72.18%\nTime elapsed: 6.21 min\nEpoch: 036/050 | Batch 000/266 | Loss: 0.3423\nEpoch: 036/050 | Batch 050/266 | Loss: 0.3257\nEpoch: 036/050 | Batch 100/266 | Loss: 0.3017\nEpoch: 036/050 | Batch 150/266 | Loss: 0.2550\nEpoch: 036/050 | Batch 200/266 | Loss: 0.3404\nEpoch: 036/050 | Batch 250/266 | Loss: 0.4620\ntraining accuracy: 86.84%\nvalid accuracy: 72.32%\nTime elapsed: 6.39 min\nEpoch: 037/050 | Batch 000/266 | Loss: 0.5645\nEpoch: 037/050 | Batch 050/266 | Loss: 0.4954\nEpoch: 037/050 | Batch 100/266 | Loss: 0.4911\nEpoch: 037/050 | Batch 150/266 | Loss: 0.3174\nEpoch: 037/050 | Batch 200/266 | Loss: 0.5089\nEpoch: 037/050 | Batch 250/266 | Loss: 0.5664\ntraining accuracy: 84.77%\nvalid accuracy: 69.67%\nTime elapsed: 6.57 min\nEpoch: 038/050 | Batch 000/266 | Loss: 0.5043\nEpoch: 038/050 | Batch 050/266 | Loss: 0.4768\nEpoch: 038/050 | Batch 100/266 | Loss: 0.4971\nEpoch: 038/050 | Batch 150/266 | Loss: 0.4369\nEpoch: 038/050 | Batch 200/266 | Loss: 0.3123\nEpoch: 038/050 | Batch 250/266 | Loss: 0.3542\ntraining accuracy: 85.97%\nvalid accuracy: 69.60%\nTime elapsed: 6.75 min\nEpoch: 039/050 | Batch 000/266 | Loss: 0.3724\nEpoch: 039/050 | Batch 050/266 | Loss: 0.4279\nEpoch: 039/050 | Batch 100/266 | Loss: 0.4045\nEpoch: 039/050 | Batch 150/266 | Loss: 0.3107\nEpoch: 039/050 | Batch 200/266 | Loss: 0.4860\nEpoch: 039/050 | Batch 250/266 | Loss: 0.5874\ntraining accuracy: 77.61%\nvalid accuracy: 61.65%\nTime elapsed: 6.93 min\nEpoch: 040/050 | Batch 000/266 | Loss: 0.3515\nEpoch: 040/050 | Batch 050/266 | Loss: 0.5365\nEpoch: 040/050 | Batch 100/266 | Loss: 0.6855\nEpoch: 040/050 | Batch 150/266 | Loss: 0.3126\nEpoch: 040/050 | Batch 200/266 | Loss: 0.7256\nEpoch: 040/050 | Batch 250/266 | Loss: 0.2944\ntraining accuracy: 83.14%\nvalid accuracy: 65.65%\nTime elapsed: 7.10 min\nEpoch: 041/050 | Batch 000/266 | Loss: 0.6638\nEpoch: 041/050 | Batch 050/266 | Loss: 0.3483\nEpoch: 041/050 | Batch 100/266 | Loss: 0.5887\nEpoch: 041/050 | Batch 150/266 | Loss: 0.3366\nEpoch: 041/050 | Batch 200/266 | Loss: 0.5161\nEpoch: 041/050 | Batch 250/266 | Loss: 0.3070\ntraining accuracy: 83.90%\nvalid accuracy: 65.80%\nTime elapsed: 7.28 min\nEpoch: 042/050 | Batch 000/266 | Loss: 0.3248\nEpoch: 042/050 | Batch 050/266 | Loss: 0.1827\nEpoch: 042/050 | Batch 100/266 | Loss: 0.6756\nEpoch: 042/050 | Batch 150/266 | Loss: 0.5219\nEpoch: 042/050 | Batch 200/266 | Loss: 0.5364\nEpoch: 042/050 | Batch 250/266 | Loss: 0.3388\ntraining accuracy: 84.42%\nvalid accuracy: 68.55%\nTime elapsed: 7.46 min\nEpoch: 043/050 | Batch 000/266 | Loss: 0.7743\nEpoch: 043/050 | Batch 050/266 | Loss: 0.3456\nEpoch: 043/050 | Batch 100/266 | Loss: 0.2986\nEpoch: 043/050 | Batch 150/266 | Loss: 0.2758\nEpoch: 043/050 | Batch 200/266 | Loss: 0.2528\nEpoch: 043/050 | Batch 250/266 | Loss: 0.7507\ntraining accuracy: 86.65%\nvalid accuracy: 70.80%\nTime elapsed: 7.64 min\nEpoch: 044/050 | Batch 000/266 | Loss: 0.3095\nEpoch: 044/050 | Batch 050/266 | Loss: 0.2352\nEpoch: 044/050 | Batch 100/266 | Loss: 0.3824\nEpoch: 044/050 | Batch 150/266 | Loss: 0.3749\nEpoch: 044/050 | Batch 200/266 | Loss: 0.4030\nEpoch: 044/050 | Batch 250/266 | Loss: 0.3378\ntraining accuracy: 88.10%\nvalid accuracy: 72.28%\nTime elapsed: 7.82 min\nEpoch: 045/050 | Batch 000/266 | Loss: 0.8190\nEpoch: 045/050 | Batch 050/266 | Loss: 0.5212\nEpoch: 045/050 | Batch 100/266 | Loss: 0.1572\nEpoch: 045/050 | Batch 150/266 | Loss: 0.3967\nEpoch: 045/050 | Batch 200/266 | Loss: 0.5509\nEpoch: 045/050 | Batch 250/266 | Loss: 0.4403\ntraining accuracy: 85.13%\nvalid accuracy: 68.48%\nTime elapsed: 8.00 min\nEpoch: 046/050 | Batch 000/266 | Loss: 0.2732\nEpoch: 046/050 | Batch 050/266 | Loss: 0.2483\nEpoch: 046/050 | Batch 100/266 | Loss: 0.3327\nEpoch: 046/050 | Batch 150/266 | Loss: 0.2813\nEpoch: 046/050 | Batch 200/266 | Loss: 0.6314\nEpoch: 046/050 | Batch 250/266 | Loss: 0.5382\ntraining accuracy: 86.71%\nvalid accuracy: 69.33%\nTime elapsed: 8.18 min\nEpoch: 047/050 | Batch 000/266 | Loss: 0.4049\nEpoch: 047/050 | Batch 050/266 | Loss: 0.2532\nEpoch: 047/050 | Batch 100/266 | Loss: 0.2059\nEpoch: 047/050 | Batch 150/266 | Loss: 0.4108\nEpoch: 047/050 | Batch 200/266 | Loss: 0.3474\nEpoch: 047/050 | Batch 250/266 | Loss: 0.1574\ntraining accuracy: 89.36%\nvalid accuracy: 73.42%\nTime elapsed: 8.36 min\nEpoch: 048/050 | Batch 000/266 | Loss: 0.5113\nEpoch: 048/050 | Batch 050/266 | Loss: 0.3233\nEpoch: 048/050 | Batch 100/266 | Loss: 0.4507\nEpoch: 048/050 | Batch 150/266 | Loss: 0.6832\nEpoch: 048/050 | Batch 200/266 | Loss: 0.6229\nEpoch: 048/050 | Batch 250/266 | Loss: 0.5291\ntraining accuracy: 80.30%\nvalid accuracy: 62.55%\nTime elapsed: 8.54 min\nEpoch: 049/050 | Batch 000/266 | Loss: 0.2178\nEpoch: 049/050 | Batch 050/266 | Loss: 0.2390\nEpoch: 049/050 | Batch 100/266 | Loss: 0.5091\nEpoch: 049/050 | Batch 150/266 | Loss: 0.5820\nEpoch: 049/050 | Batch 200/266 | Loss: 0.4851\nEpoch: 049/050 | Batch 250/266 | Loss: 0.4157\ntraining accuracy: 89.29%\nvalid accuracy: 73.12%\nTime elapsed: 8.72 min\nEpoch: 050/050 | Batch 000/266 | Loss: 0.2143\nEpoch: 050/050 | Batch 050/266 | Loss: 0.2270\nEpoch: 050/050 | Batch 100/266 | Loss: 0.2661\nEpoch: 050/050 | Batch 150/266 | Loss: 0.2877\nEpoch: 050/050 | Batch 200/266 | Loss: 0.3744\nEpoch: 050/050 | Batch 250/266 | Loss: 0.2662\ntraining accuracy: 89.80%\nvalid accuracy: 73.53%\nTime elapsed: 8.90 min\nTotal Training Time: 8.90 min\nTest accuracy: 73.72%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Testing","metadata":{}},{"cell_type":"code","source":"import spacy\n\n\nnlp = spacy.blank(\"en\")\n\ndef predict_sentiment(model, sentence):\n\n    model.eval()\n    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n    length = [len(indexed)]\n    tensor = torch.LongTensor(indexed).to(DEVICE)\n    tensor = tensor.unsqueeze(1)\n    length_tensor = torch.LongTensor(length)\n    prediction = torch.nn.functional.softmax(model(tensor), dim=1)\n    return prediction[0][1].item()\n\nprint('Probability positive:')\npredict_sentiment(model, \"This is such an awesome movie, I really love it!\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T06:41:12.754558Z","iopub.execute_input":"2022-05-02T06:41:12.754811Z","iopub.status.idle":"2022-05-02T06:41:12.955525Z","shell.execute_reply.started":"2022-05-02T06:41:12.754781Z","shell.execute_reply":"2022-05-02T06:41:12.954732Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"Probability positive:\n","output_type":"stream"},{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"0.9224046468734741"},"metadata":{}}]},{"cell_type":"code","source":"print('Probability positive:')\npredict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T06:41:16.146385Z","iopub.execute_input":"2022-05-02T06:41:16.146908Z","iopub.status.idle":"2022-05-02T06:41:16.154922Z","shell.execute_reply.started":"2022-05-02T06:41:16.146870Z","shell.execute_reply":"2022-05-02T06:41:16.154210Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Probability positive:\n","output_type":"stream"},{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"0.01575806923210621"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}